{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj7dpzumo9ZIKYOWec9wpx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muziejus/coms-w4995-applied-machine-learning-project/blob/main/notebooks/combine_sentiment_and_financial_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Sentiment and Financial Data\n",
        "\n",
        "This notebook takes the merged financial dataset prepared by Opel and Meunier along with the sentiment analysis results of each individual company’s articles (with code provided by Mai and Sá Pereira) and generates five individual parquet files, one for each company.\n",
        "\n",
        "This code was prepared by Sá Pereira and accomplishes these tasks before merging:\n",
        "\n",
        "1. Aggregates the per-article sentiment data by publication date to produce weighted averages and errors as well as rolling means for sentiment.\n",
        "\n",
        "2. Breaks apart the financial data back into per-company structures and adds rolling means for price and volume as well as Bollinger bands. Then we add a numerical target (price) and a buy/sell classifaction target (1, 0).\n",
        "\n",
        "The merged files are on GitHub in `data/` and have the name `<company>_merged_data.parquet`"
      ],
      "metadata": {
        "id": "BWBwGIbUSRen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Ho8zrncIl-x6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Constants\n",
        "\n",
        "Here we intialize the constants for loading data, our list of companies, and initialize the financial DataFrame, which has indicators for all five companies as well as external indicators merged together. The columns are broken apart and described below. The variable `sentiment_article_number` pertains to the (maximum) number of articles analyzed. We analyzed them in batches of 25,000 articles at once for each company."
      ],
      "metadata": {
        "id": "aNxeqyt2pQnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_data_url = \"https://github.com/muziejus/coms-w4995-applied-machine-learning-project/raw/refs/heads/main/data\"\n",
        "financial_data_url = \"financial_data\"\n",
        "sentiment_article_number = 75000\n",
        "sentiment_data_url = f\"sentiment_data/analyzed_{sentiment_article_number}\"\n",
        "sentiment_file_name_tail = f\"_x_{sentiment_article_number}.parquet\"\n",
        "\n",
        "companies = [\"dltr\", \"lulu\", \"ulta\", \"wba\", \"wmt\"]"
      ],
      "metadata": {
        "id": "sdVgR3_Jmark"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin_df = pd.read_csv(f\"{root_data_url}/{financial_data_url}/merged_data.csv\")"
      ],
      "metadata": {
        "id": "tUCs_V9PpG1T"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create aggregated sentiment data\n",
        "\n",
        "For each company, we read in the parquet file from GitHub that has the results of our sentiment analysis. The columns in the parquet file are:\n",
        "\n",
        "Column | Type | Description\n",
        "---|---|---\n",
        "`index` | int | A naive index created during aggregation.\n",
        "`goid` | int | ProQuest’s globally unique identifier for the article in question.\n",
        "`date`| str | The article’s publication date (`%Y-%m-%d`).\n",
        "`tokens` | int | A naive (breaking on whitespace) count of tokens in the article.\n",
        "`corpus` | str | The corpus from which the article comes. Used previously in aggregation.\n",
        "`daily_article_count` | int | A previously calculated count of all the articles in the corpus from that day.\n",
        "`daily_token_sum` | int | A sum of all the (naive) tokens from all the articles in the corpus from that day.\n",
        "`text_sentiment` | float | The overall average sentiment for that article, from (-1, 1) with negative numbers corresponding to negative sentiments and positive with positive.\n",
        "`text_error` | float | The weighted inverse error in analysis. Higher is more confident in classification.\n",
        "`text_input_tokens` | int | The number of tokens as analyzed by RoBERTa’s byte-pair encoding tokenizer.\n",
        "\n",
        "We group the data by date and create a new DataFrame with the following columns:\n",
        "\n",
        "Column | Type | Description\n",
        "---|---|---\n",
        "(index)| datetime | The articles’ publication date.\n",
        "`analyzed_bpe_tokens`| int | The number of tokens as analyzed by RoBERTa’s BPE tokenizer for all the articles analyzed for the day.\n",
        "`weighted_sentiment` | float | The mean sentiment for the day (from (-1, 1) as above), weighted by each individual article’s length.\n",
        "`weighted_error` | float | The mean inverse error for the day, weighted by each individual article’s length. Higher is more confident.\n",
        "`analyzed_naive_tokens` | int | The number of naive tokens (words separated by whitespace) analyzed for the day.\n",
        "`daily_naive_token_sum` | int | The total number of naive tokens available for the day.\n",
        "`analyzed_article_count` | int | The number of articles analyzed for the day.\n",
        "`daily_article_count` | int | The number of available articles for the day.\n",
        "`sentiment_3d_rolling_mean` | float | Three-day rolling average for sentiment.\n",
        "`sentiment_7d_rolling_mean` | float | Seven-day rolling average for sentiment.\n",
        "`sentiment_14d_rolling_mean` | float | Fourteen-day rolling average for sentiment."
      ],
      "metadata": {
        "id": "U6gCIQ0uqMsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_avg(row, value_column, weight_column):\n",
        "    return (row[value_column] * row[weight_column]).sum() / row[weight_column].sum()\n",
        "\n",
        "def aggregate_sentiment(df):\n",
        "  agg_sent_df = (\n",
        "    df.groupby(\"date\")\n",
        "    .agg(\n",
        "        analyzed_bpe_tokens = (\"text_input_tokens\", lambda row: row.sum().astype(int)),\n",
        "        weighted_sentiment=(\n",
        "            \"text_input_tokens\",\n",
        "            lambda row: weighted_avg(df.loc[row.index], \"text_sentiment\", \"text_input_tokens\")\n",
        "            ),\n",
        "        weighted_error=(\n",
        "            \"text_input_tokens\",\n",
        "            lambda row: weighted_avg(df.loc[row.index], \"text_error\", \"text_input_tokens\")\n",
        "            ),\n",
        "        analyzed_naive_tokens = (\"tokens\", \"sum\"),\n",
        "        daily_naive_token_sum = (\"daily_token_sum\", \"first\"),\n",
        "        analyzed_article_count = (\"index\", \"count\"),\n",
        "        daily_article_count = (\"daily_article_count\", \"first\")\n",
        "    )\n",
        "  )\n",
        "  agg_sent_df.reset_index(inplace=True)\n",
        "  agg_sent_df[\"date\"] = pd.to_datetime(agg_sent_df.date)\n",
        "  agg_sent_df.sort_values(\"date\", inplace=True) # probably redundant\n",
        "  agg_sent_df.set_index('date', inplace=True)\n",
        "\n",
        "  for window in [3, 7, 14]:\n",
        "    agg_sent_df[f\"sentiment_{window}d_rolling_mean\"] = agg_sent_df.weighted_sentiment.rolling(window).mean()\n",
        "\n",
        "  return agg_sent_df"
      ],
      "metadata": {
        "id": "A9ge6ZcGpA-m"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Company-Specific Financial Data\n",
        "\n",
        "For each company, we use the financial dataframe to create a new data frame specific to the company. It has these columns:\n",
        "\n",
        "Column | Type | Description\n",
        "---|---|---\n",
        "(index)| datetime | The date.\n",
        "`open` | float | The opening price.\n",
        "`high` | float | The high price.\n",
        "`low` | float | The low price.\n",
        "`close` | float | The closing price.\n",
        "`volume` | float | The number of shares traded.\n",
        "`dividends` | float | Dividends paid (if any).\n",
        "`stock_splits` | float | Stock splits (if any).\n",
        "`cpi` | float| Consumer Price Index for all urban consumers. Not seasonally adjusted\n",
        "`pce` | float| Personal Consumption Expenditures index.\n",
        "`ppi` | float| Producer Price Index for All Commodities.\n",
        "`eci` | float| Employment Cost Index for All Civilian Workers.\n",
        "`gdp` |float | GDP Deflator. Adjusts nominal GDP to real GDP.\n",
        "`unemployment` |float | Unemployment Rate.\n",
        "`manufacturing` | float | Manufacturing and Construction Employment.\n",
        "`sp500` | float | Standard & Poor’s 500 Index.\n",
        "`price_3d_rolling_mean` | float | Rolling three-day price mean.\n",
        "`price_3d_rolling_std` | float | Rolling three-day price standard deviation.\n",
        "`volume_3d_rolling_mean` | float | Rolling three-day volume mean.\n",
        "`volume_3d_rolling_std` | float | Rolling three-day volume standard deviation.\n",
        "`bollinger_3d_upper_band` | float | Bollinger three-day upper band.\n",
        "`bollinger_3d_lower_band` | float | Bollinger three-day lower band.\n",
        "`price_7d_rolling_mean` | float | Rolling seven-day price mean.\n",
        "`price_7d_rolling_std` | float | Rolling seven-day price standard deviation.\n",
        "`volume_7d_rolling_mean` | float | Rolling seven-day volume mean.\n",
        "`volume_7d_rolling_std` | float | Rolling seven-day volume standard deviation.\n",
        "`bollinger_7d_upper_band` | float | Bollinger seven-day upper band.\n",
        "`bollinger_7d_lower_band` | float | Bollinger seven-day lower band.\n",
        "`price_14d_rolling_mean` | float | Rolling 14-day price mean.\n",
        "`price_14d_rolling_std` | float | Rolling 14-day price standard deviation.\n",
        "`volume_14d_rolling_mean` | float | Rolling 14-day volume mean.\n",
        "`volume_14d_rolling_std` | float | Rolling 14-day volume standard deviation.\n",
        "`bollinger_14d_upper_band` | float | Bollinger 14-day upper band.\n",
        "`bollinger_14d_lower_band` | float | Bollinger 14-day lower band.\n",
        "`target_price` | float | Next day’s price.\n",
        "`target` | int | Buy or sell recommendation (1 or 0)\n"
      ],
      "metadata": {
        "id": "08-xFtPsMqpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_company_financial_data(company, fin_df):\n",
        "  financial_columns = [\n",
        "      \"Date\",\n",
        "      f\"Open_{company}\",\n",
        "      f\"High_{company}\",\n",
        "      f\"Low_{company}\",\n",
        "      f\"Close_{company}\",\n",
        "      f\"Volume_{company}\",\n",
        "      f\"Dividends_{company}\",\n",
        "      f\"Stock Splits_{company}\",\n",
        "      \"CPIAUCSL\", \"PCE\",\n",
        "      \"PPIACO\", \"ECIALLCIV\", \"GDPDEF\", \"UNRATE\", \"MCUMFN\", \"SP500\"\n",
        "  ]\n",
        "  new_column_names = {\n",
        "      \"Date\": \"date\",\n",
        "      f\"Open_{company}\": \"open\",\n",
        "      f\"High_{company}\": \"high\",\n",
        "      f\"Low_{company}\": \"low\",\n",
        "      f\"Close_{company}\": \"close\",\n",
        "      f\"Volume_{company}\": \"volume\",\n",
        "      f\"Dividends_{company}\": \"dividends\",\n",
        "      f\"Stock Splits_{company}\": \"stock_splits\",\n",
        "      \"CPIAUCSL\": \"cpi\",\n",
        "      \"PCE\": \"pce\",\n",
        "      \"PPIACO\": \"ppi\",\n",
        "      \"ECIALLCIV\": \"eci\",\n",
        "      \"GDPDEF\": \"gdp\",\n",
        "      \"UNRATE\": \"unemployment\",\n",
        "      \"MCUMFN\": \"manufacturing\",\n",
        "      \"SP500\": \"sp500\"\n",
        "  }\n",
        "  company_fin_df = fin_df[financial_columns]\n",
        "  company_fin_df = company_fin_df.rename(columns=new_column_names)\n",
        "\n",
        "  company_fin_df[\"date\"] = pd.to_datetime(company_fin_df[\"date\"])\n",
        "  company_fin_df.set_index(\"date\", inplace=True)\n",
        "\n",
        "  for window in [3, 7, 14]:\n",
        "    company_fin_df[f\"price_{window}d_rolling_mean\"] = company_fin_df.close.rolling(window).mean()\n",
        "    company_fin_df[f\"price_{window}d_rolling_std\"] = company_fin_df.close.rolling(window).std()\n",
        "    company_fin_df[f\"volume_{window}d_rolling_mean\"] = company_fin_df.volume.rolling(window).mean()\n",
        "    company_fin_df[f\"volume_{window}d_rolling_std\"] = company_fin_df.volume.rolling(window).std()\n",
        "    # Bollinger Bands\n",
        "    std_dev = 2\n",
        "    company_fin_df[f\"bollinger_{window}d_upper_band\"] = (\n",
        "        company_fin_df[f\"price_{window}d_rolling_mean\"] + (company_fin_df[f\"price_{window}d_rolling_std\"] * std_dev)\n",
        "    )\n",
        "    company_fin_df[f\"bollinger_{window}d_lower_band\"] = (\n",
        "        company_fin_df[f\"price_{window}d_rolling_mean\"] - (company_fin_df[f\"price_{window}d_rolling_std\"] * std_dev)\n",
        "    )\n",
        "\n",
        "  company_fin_df[\"target_price\"] = company_fin_df.close.shift(-1)\n",
        "  company_fin_df[\"target\"] = (company_fin_df.close.shift(-1) > company_fin_df.close).astype(int)\n",
        "\n",
        "  return company_fin_df\n"
      ],
      "metadata": {
        "id": "5fH0ynrsOrui"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterate over All Five Companies\n",
        "\n",
        "We create a DataFrame for each company that merges the sentiment and financial information and then save it as a parquet file for subsequent committing to GitHub."
      ],
      "metadata": {
        "id": "u-AC-XGURV0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for company in companies:\n",
        "  sent_df = pd.read_parquet(f\"{root_data_url}/{sentiment_data_url}/{company}{sentiment_file_name_tail}\")\n",
        "  agg_sent_df = aggregate_sentiment(sent_df)\n",
        "  company_fin_df = create_company_financial_data(company, fin_df)\n",
        "  merged_df = pd.merge(agg_sent_df, company_fin_df, left_index=True, right_index=True)\n",
        "  merged_df.to_parquet(f\"{company}_merged_data.parquet\")"
      ],
      "metadata": {
        "id": "rCKRdPJpE0TT"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Yt4YuWQCowx"
      },
      "execution_count": 85,
      "outputs": []
    }
  ]
}