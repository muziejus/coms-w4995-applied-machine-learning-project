{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709acf6f",
   "metadata": {},
   "source": [
    "# Prepare Texts\n",
    "\n",
    "This notebook combines code by Moacir P. de SÃ¡ Pereira with default TDMStudio code provded by ProQuest/Clarivate.\n",
    "\n",
    "It assumes the existence of a set of corpora available as various directories like `./data/{corpus_name}`, each of which contains $n$ xml files of the name `{goid}.xml`, where `goid` is a global id used by ProQuest for their articles.\n",
    "\n",
    "For each corpus, it generates a set of csv files of at most 10,000 records, where each row corresponds to an article in the corpus. The csvs are written to `./dataframe_files` with the name `{corpus}_nnn.csv`. The csvs have the following columns:\n",
    "\n",
    "- `goid`: Int. As above\n",
    "- `title`: Str. The headline of the article\n",
    "- `date`: Str. The publication date, in `YYYY-MM-DD` format\n",
    "- `publisher`: Str. The article's publisher\n",
    "- `pub_title`: Str. The title of the publication\n",
    "- `author`: Str. The display name of the author, when available\n",
    "- `tokens`: Int. A naive word count, derived from splitting the full text on whitespace.\n",
    "    \n",
    "The csvs are subsequently used in the `concatenate-corpora` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for parsing data\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip html tags from text portion\n",
    "def strip_html_tags(text):\n",
    "    stripped = BeautifulSoup(text).get_text().replace('\\n', ' ').replace('\\\\', '').strip()\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c219c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getxmlcontent(corpus_path, file, strip_html=True):\n",
    "    try:\n",
    "        result = {\n",
    "            \"goid\": None,\n",
    "            \"title\": None,\n",
    "            \"date\": None,\n",
    "            \"publisher\": None,\n",
    "            \"pub_title\": None,\n",
    "            \"author\": None,\n",
    "            \"text\": None\n",
    "        }\n",
    "        \n",
    "        tree = etree.parse(corpus_path + file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        if root.find(\".//ISOExpansion\").text == \"English\": # Only use English articles\n",
    "            if root.find('.//GOID') is not None:\n",
    "                result[\"goid\"] = root.find('.//GOID').text\n",
    "            if root.find('.//Title') is not None:\n",
    "                result[\"title\"] = root.find('.//Title').text\n",
    "            if root.find('.//PubFrosting/Title') is not None:\n",
    "                result[\"pub_title\"] = root.find('.//PubFrosting/Title').text\n",
    "            if root.find('.//NumericDate') is not None:\n",
    "                result[\"date\"] = root.find('.//NumericDate').text\n",
    "            if root.find('.//PublisherName') is not None:\n",
    "                result[\"publisher\"] = root.find('.//PublisherName').text\n",
    "            if root.find('.//Author/NormalizedDisplayForm') is not None:\n",
    "                result[\"author\"] = root.find('.//Author/NormalizedDisplayForm').text\n",
    "            # Check for text in various potential places in the XML tree\n",
    "            if root.find('.//FullText') is not None:\n",
    "                result[\"text\"] = root.find('.//FullText').text\n",
    "            elif root.find('.//HiddenText') is not None:\n",
    "                result[\"text\"] = root.find('.//HiddenText').text\n",
    "            elif root.find('.//Text') is not None:\n",
    "                result[\"text\"] = root.find('.//Text').text\n",
    "\n",
    "            # Strip html from text portion\n",
    "            if result[\"text\"] is not None and strip_html == True:\n",
    "                result[\"text\"] = strip_html_tags(result[\"text\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing file {file}: {e}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_files(corpus, sample_size = None, batch_size=10000):\n",
    "    corpus_path = f\"/home/ec2-user/SageMaker/data/{corpus}/\"\n",
    "    files = os.listdir(corpus_path)\n",
    "    if sample_size:\n",
    "        files = random.sample(files, sample_size)\n",
    "    \n",
    "    rows = []\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "        file_count = len(files)\n",
    "        result = getxmlcontent(corpus_path, file, strip_html=True)\n",
    "        rows.append(result)\n",
    "        if (i != 0 and i % batch_size == 0) or i == file_count - 1 :\n",
    "            df = pd.DataFrame(rows)\n",
    "            # Drop rows with no text.\n",
    "            df = df.dropna(subset=['text'])\n",
    "            # Naively calculate a word count for each article.\n",
    "            df[\"tokens\"] = df[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "            # Drop the text column.\n",
    "            df = df.drop(columns=[\"text\"])\n",
    "            \n",
    "            # Write csv.\n",
    "            file_name = f\"./dataframe_files/{corpus}_{str(i//batch_size).zfill(3)}.csv\"\n",
    "            df.to_csv(file_name)\n",
    "            print(f\"Wrote {file_name}\")\n",
    "            rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ca64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = [\n",
    "    \"dollar-tree\",\n",
    "    \"lululemon\",\n",
    "    \"ulta\",\n",
    "    \"walgreens\",\n",
    "    \"walmart\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for corpus in corpora:\n",
    "    print(f\"Starting {corpus}\")\n",
    "    df = prepare_files(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
