{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9PIiVvcDt4"
      },
      "source": [
        "# Consolidate Analysis Files\n",
        "\n",
        "This notebook is by Moacir P. de SÃ¡ Pereira.\n",
        "\n",
        "This notebook consolidates the files in `./analyzed_files/`, which take the form of `{corpus}_nnn.parquet`. To account for the tendency of TDM Studio to shut itself off, we saved our work in 1,000 article batches and generated 20 of those files for each corpus with every run until the system shut off. Given that the server auto shuts down after two days and we can analyze about 500 articles an hour, we would never complete the full run of 20 batches for all corpora.\n",
        "\n",
        "The consolidated files are saved as `./analyzed_full_parquet_files/{corpus}_x_{file_count}000.parquet`, where `file_count` indicates how deeply into the analysis we got. By the end of this project, we had accumulated 100,000 analyzed articles for the two largest corpora and exhausted the smallest three.\n",
        "\n",
        "Next, we imported the analyzed full parquet files into our GitHub repo and renamed them `{ticker}_sent.parquet`, where `ticker` is the company's stock symbol and not the name of the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnbcjKrF97xQ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSRLmdXm97xR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqYaocQ97xR"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxT0mWR597xR"
      },
      "outputs": [],
      "source": [
        "corpora = [\n",
        "    \"walgreens\",\n",
        "    \"walmart\",\n",
        "    \"dollar-tree\",\n",
        "    \"lululemon\",\n",
        "    \"ulta\"\n",
        "]\n",
        "\n",
        "root_path = \"/home/ec2-user/SageMaker\"\n",
        "analyzed_files_path = f\"{root_path}/analyzed_batch_parquet_files\"\n",
        "analyzed_full_parquet_path = f\"{root_path}/analyzed_full_parquet_files\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHXanNjw97xR"
      },
      "outputs": [],
      "source": [
        "# Get files with 100,000 analyzed articles (at least)\n",
        "file_count = 100\n",
        "for corpus in corpora:\n",
        "    dfs = []\n",
        "    for i in range(1, file_count + 1):\n",
        "        try:\n",
        "            path = f\"{analyzed_files_path}/{corpus}_{str(i).zfill(3)}.parquet\"\n",
        "            df_fragment = pd.read_parquet(path)\n",
        "            dfs.append(df_fragment)\n",
        "        except:\n",
        "            continue\n",
        "    df = pd.concat(dfs)\n",
        "    df.to_parquet(f\"{analyzed_full_parquet_path}/{corpus}_x_{file_count}000.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WitE77cr97xS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "moacir-py",
      "language": "python",
      "name": "moacir-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}