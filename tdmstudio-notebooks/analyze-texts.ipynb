{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9PIiVvcDt4"
      },
      "source": [
        "# Analyze Texts\n",
        "\n",
        "This notebook combines code by Vi Mai and Moacir P. de Sá Pereira with default TDMStudio code provded by ProQuest/Clarivate.\n",
        "Mai wrote the Huggingface interface and Sá Pereira put everything together and wrote the general workflow.\n",
        "\n",
        "This notebook assumes the existence of two different datasets:\n",
        "\n",
        "1. A set of corpora available as various directories like `./data/{corpus_name}`, each of which contains $n$ xml files of the name `{goid}.xml`, where `goid` is a global id used by ProQuest for their articles\n",
        "\n",
        "2. A set of parquet files in `./full_parquets`, each of the name `{corpus}.parquet`. The files are all concatenated versions of the chunked csvs in `./dataframe_files` and drop certain columns, use others, and omit rows (articles) that are overly long or have weekend dates of publication. These files were created in the `concatenate-corpora` notebook and have these columns:\n",
        "\n",
        "- `index`: Int. A consecutive index.\n",
        "- `goid`: Int. ProQuests global ID for their articles.\n",
        "- `date`: DT. The publication date, in datetime format.\n",
        "- `tokens`: Int. A naive word count, derived from splitting the full text on whitespace.\n",
        "- `corpus`: Str. The corpus name. This is used in the next notebook.\n",
        "- `daily_article_count`: Int. The number of articles in the corpus for that day.\n",
        "- `daily_token_sum`: Int. The sum of naive tokens in the corpus for that day.\n",
        "\n",
        "In this notebook, we iterate over each corpus and chunk each corpus back into 1000 article sized chunks. We analyze the sentiment of each article's full text, using the [distilroberta-finetuned-financial-news-sentiment-analysis](https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis) model by Miguel Romero, downloaded from Huggingface.\n",
        "\n",
        "We derive three new columns:\n",
        "\n",
        "- `text_sentiment`: Float. A weighted average sentiment for the article. The score range is (-1, 1) to account for negative, neutral, and positive sentiment (positive numbers indicate positive sentiment).\n",
        "- `text_error`: Float. A weighted inverse squared error that indicates the model’s confidence in its labeling. The higher this value is, the more confident the model is of its analysis.\n",
        "- `text_input_tokens`: Int. The number of input tokens used by the model. The model uses the RoBERTa tokenizer, which uses byte pair encoding for subword tokenization. As a result, this number is typically larger than the `tokens` column but gives a sense of how many chunks the model split the article text into (the model can take only 512 tokens at a time).\n",
        "\n",
        "Each new batch dataframe with the sentiment scores is saved to a new parquet file named `./analyzed_files/{corpus}_nnn.parquet`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvFi4Af41kn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUi2AhCH41kn"
      },
      "outputs": [],
      "source": [
        "%conda update -n base -c conda-forge conda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iid2-Ari41ko"
      },
      "outputs": [],
      "source": [
        "%conda install pyarrow=15.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gef0SF1w41ko"
      },
      "outputs": [],
      "source": [
        "%conda install pandas=2.2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0uG16lk41ko"
      },
      "outputs": [],
      "source": [
        "%conda install pytorch=2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-5OWCrN41ko"
      },
      "outputs": [],
      "source": [
        "%conda install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KmmA7Rx41ko"
      },
      "outputs": [],
      "source": [
        "%conda install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_DhMX3q41ko"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lxml import etree\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-qRRHvE41ko"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqOfoOUW41ko"
      },
      "outputs": [],
      "source": [
        "corpora = [\n",
        "    \"dollar-tree\",\n",
        "    \"lululemon\",\n",
        "    \"ulta\",\n",
        "    \"walgreens\",\n",
        "    \"walmart\"\n",
        "]\n",
        "\n",
        "root_path = \"/home/ec2-user/SageMaker\"\n",
        "full_parquets_path = f\"{root_path}/full_fixed_parquet_files\"\n",
        "analyzed_files_path = f\"{root_path}/analyzed_batch_parquet_files\"\n",
        "analyzed_full_parquet_path = f\"{root_path}/analyzed_full_parquet_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GejeWSQ41ko"
      },
      "source": [
        "## Text Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1auwGRd941ko"
      },
      "outputs": [],
      "source": [
        "# Function to read a single text from a single xml file.\n",
        "def get_text(corpus, goid):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        tree = etree.parse(f\"{root_path}/data/{corpus}/{goid}.xml\")\n",
        "        root = tree.getroot()\n",
        "        if root.find('.//FullText') is not None:\n",
        "            text = root.find('.//FullText').text\n",
        "        elif root.find('.//HiddenText') is not None:\n",
        "            text = root.find('.//HiddenText').text\n",
        "        elif root.find('.//Text') is not None:\n",
        "            text = root.find('.//Text').text\n",
        "\n",
        "        text = BeautifulSoup(text).get_text().replace('\\n', ' ').replace('\\\\', '').strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while parsing file {file}: {e}\")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lqq5kT341ko"
      },
      "outputs": [],
      "source": [
        "class SentimentAnalysisEngine:\n",
        "    def __init__(self, huggingface_model = \"distilroberta-finetuned-financial-news-sentiment-analysis\"):\n",
        "        self.model_path = f\"models/{huggingface_model}\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only = True)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path, local_files_only = True)\n",
        "        self.max_length = 512 # Huggingface maximum\n",
        "\n",
        "    def analyze(self, text):\n",
        "        # Adapted from https://github.com/huggingface/transformers/issues/9321\n",
        "        averages = []\n",
        "        errors = []\n",
        "        chunk_sizes = []\n",
        "\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"][0]\n",
        "        length = len(input_ids)\n",
        "\n",
        "        # Chunk input tensor into max_length-sized pieces.\n",
        "        chunks = [\n",
        "            input_ids[i:i + self.max_length]\n",
        "            for i in range(0, length, self.max_length)\n",
        "        ]\n",
        "\n",
        "        for chunk in chunks:\n",
        "            chunk_inputs={k: v[:, :len(chunk)].reshape(1, -1) for k, v in inputs.items()}\n",
        "            chunk_inputs[\"input_ids\"] = chunk.reshape(1, -1)\n",
        "\n",
        "            preds = self.model(**chunk_inputs)\n",
        "            weights = torch.softmax(preds.logits, dim=1) # could move to cuda in theory\n",
        "            # print(torch.allclose(torch.sum(weights, dim=-1), torch.tensor(1.0)))  # Returns True if the sum is 1\n",
        "            values = torch.linspace(-1, 1, steps=3)\n",
        "            average = torch.dot(weights[0], values) # Don't need to divide. Weights sum to 1 in softmax\n",
        "            averages.append(average.item())\n",
        "\n",
        "            deviations = (values - average) ** 2\n",
        "            variance = torch.dot(weights[0], deviations)\n",
        "            error = torch.sqrt(variance)\n",
        "            errors.append(error.item())\n",
        "\n",
        "            chunk_sizes.append(len(chunk))\n",
        "\n",
        "        chunk_sizes = torch.tensor(chunk_sizes, dtype=torch.float32)\n",
        "        chunk_weighted_averages = torch.tensor(\n",
        "            averages, dtype=torch.float32) * chunk_sizes\n",
        "        errors = torch.tensor(errors, dtype=torch.float32)\n",
        "        inverse_squared_errors = 1 / errors**2\n",
        "        chunk_weighted_errors = inverse_squared_errors * chunk_sizes\n",
        "        weighted_average = torch.sum(\n",
        "            chunk_weighted_averages * inverse_squared_errors\n",
        "            ) / torch.sum(chunk_weighted_errors)\n",
        "        weighted_error = torch.sqrt(1.0 / torch.sum(1.0 / chunk_weighted_errors))\n",
        "\n",
        "        # Return the final weighted average, final weighted error (higher = more confident),\n",
        "        # and total number of input tokens.\n",
        "        return weighted_average.item(), weighted_error.item(), length # total number of tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhAU7y2m41kp"
      },
      "source": [
        "### Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m4rxYqw41kp"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer = SentimentAnalysisEngine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62bwGJNG41kp",
        "outputId": "6d0ab8d9-8b20-48bc-a12e-54bd191e276e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-0.9898356199264526, 30.163711547851562, 10)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentiment_analyzer.analyze(\"This stock is going to crash very soon\")\n",
        "# -0.9898354917397236"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LlaXj2J41kp",
        "outputId": "de7ae700-7e8a-4abc-f837-500d93c9bbf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.9984613060951233, 69.96183013916016, 10)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentiment_analyzer.analyze(\"This stock is going to soar very soon\")\n",
        "# 0.9984612356163491"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKf2acdt41kp",
        "outputId": "1d6b2729-e718-4491-e694-d305ac1f6682"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-3.9350346924038604e-05, 237.02548217773438, 8)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentiment_analyzer.analyze(\"This stock is going to blah\")\n",
        "# -3.935034447049273e-05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0spox7q741kp"
      },
      "source": [
        "## Iterate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHK-KtQh41kp"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer = SentimentAnalysisEngine()\n",
        "\n",
        "def analyze_row(row):\n",
        "    text = get_text(row.corpus, row.goid) # Why we added the `corpus` column.\n",
        "    text_sentiment, text_error, text_input_tokens = sentiment_analyzer.analyze(text)\n",
        "    return text_sentiment, text_error, text_input_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0bs-HQA41kp"
      },
      "outputs": [],
      "source": [
        "# This function returns the last index analyzed\n",
        "# so we know where to begin our search in the full corpus\n",
        "def get_last_index(corpus):\n",
        "    analyzed_files = sorted([file for file in os.listdir(analyzed_files_path) if corpus in file])\n",
        "    path = f\"{analyzed_files_path}/{corpus}_{str(len(analyzed_files)).zfill(3)}.parquet\"\n",
        "    try:\n",
        "        last_df = pd.read_parquet(path)\n",
        "        last_index = int(last_df.tail(1)[\"index\"].iloc[0])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Couldn’t find {path}. This is probably OK.\")\n",
        "        last_index = -1\n",
        "\n",
        "    return last_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtYydT9A41kp"
      },
      "outputs": [],
      "source": [
        "def iterate_on_corpus(corpus, batch_size):\n",
        "    starting_index = get_last_index(corpus) + 1\n",
        "    stopping_index = starting_index + batch_size\n",
        "    df = pd.read_parquet(f\"{full_parquets_path}/{corpus}.parquet\")\n",
        "    df_length = len(df)\n",
        "    if starting_index >= df_length - 1:\n",
        "        print(f\"No remaining articles for {corpus}\")\n",
        "        return True\n",
        "    if stopping_index >= df_length - 1:\n",
        "        print(f\"Final run for {corpus}\")\n",
        "        stopping_index = df_length - 1\n",
        "\n",
        "    batch_df = df[starting_index:stopping_index].copy()\n",
        "    batch_df[[\n",
        "        \"text_sentiment\",\n",
        "        \"text_error\",\n",
        "        \"text_input_tokens\",\n",
        "    ]] = batch_df.progress_apply(lambda row: analyze_row(row), axis=1, result_type=\"expand\")\n",
        "    path = f\"{analyzed_files_path}/{corpus}_{str(starting_index // batch_size + 1).zfill(3)}.parquet\"\n",
        "    batch_df.to_parquet(path)\n",
        "    print(f\"Wrote {path}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0LR9XLY41kp",
        "outputId": "b43fcc6d-5a98-46de-a34f-d4933fc832ba",
        "colab": {
          "referenced_widgets": [
            "96cdfce573d74565804c1ce2c78e51ee",
            "cb074fa7751d4c5f872b7264c64a18a2",
            "35856e0368d2423a8095f73ca051f90d",
            "98455a3359e34d42919bb4df587ef92f",
            "dea2956448b64f4ea10d3b7fd4f4eba1",
            "52013f5b43994c27a80f961d2946236d",
            "ebb09fc017794b39822487454cc2a8f4",
            "9ef64b4cd5484ca8976543cc2e1d5c07",
            "60f65d888b4a4dd89273dac2fd540938",
            "c3649a1a4cc44dd789b6be6aef46596e",
            "f0a5e22a8c714cb1a46d289b5715cd8c",
            "15d3ff6fc2f6491483d2e70d9ae2c0e9",
            "90b8bb4e37864af2b0e0485447c60219",
            "dbce829d28d5437b8cc49a9f2497e267",
            "b9514a92f14142a58554b5439522f37c"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Couldn’t find /home/ec2-user/SageMaker/analyzed_batch_parquet_files/dollar-tree_000.parquet. This is probably OK.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96cdfce573d74565804c1ce2c78e51ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1352 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/dollar-tree_001.parquet\n",
            "Couldn’t find /home/ec2-user/SageMaker/analyzed_batch_parquet_files/lululemon_000.parquet. This is probably OK.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb074fa7751d4c5f872b7264c64a18a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/lululemon_001.parquet\n",
            "Couldn’t find /home/ec2-user/SageMaker/analyzed_batch_parquet_files/ulta_000.parquet. This is probably OK.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35856e0368d2423a8095f73ca051f90d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/ulta_001.parquet\n",
            "Couldn’t find /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walgreens_000.parquet. This is probably OK.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98455a3359e34d42919bb4df587ef92f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walgreens_001.parquet\n",
            "Couldn’t find /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walmart_000.parquet. This is probably OK.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dea2956448b64f4ea10d3b7fd4f4eba1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walmart_001.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52013f5b43994c27a80f961d2946236d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/dollar-tree_002.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebb09fc017794b39822487454cc2a8f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/lululemon_002.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ef64b4cd5484ca8976543cc2e1d5c07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/ulta_002.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60f65d888b4a4dd89273dac2fd540938",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walgreens_002.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3649a1a4cc44dd789b6be6aef46596e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walmart_002.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0a5e22a8c714cb1a46d289b5715cd8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/dollar-tree_003.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15d3ff6fc2f6491483d2e70d9ae2c0e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/lululemon_003.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90b8bb4e37864af2b0e0485447c60219",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/ulta_003.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbce829d28d5437b8cc49a9f2497e267",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walgreens_003.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9514a92f14142a58554b5439522f37c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /home/ec2-user/SageMaker/analyzed_batch_parquet_files/walmart_003.parquet\n"
          ]
        }
      ],
      "source": [
        "tqdm.pandas(desc=\"Progress\")\n",
        "\n",
        "batch_size = 3\n",
        "\n",
        "for i in range(3):\n",
        "    for corpus in corpora:\n",
        "        iterate_on_corpus(corpus, batch_size)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbxa0_Rg41kp",
        "outputId": "6c13481b-cb54-4587-8561-5f5ab0acac6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>goid</th>\n",
              "      <th>date</th>\n",
              "      <th>tokens</th>\n",
              "      <th>corpus</th>\n",
              "      <th>daily_article_count</th>\n",
              "      <th>daily_token_sum</th>\n",
              "      <th>text_sentiment</th>\n",
              "      <th>text_error</th>\n",
              "      <th>text_input_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>2547227213</td>\n",
              "      <td>2019-08-02</td>\n",
              "      <td>579</td>\n",
              "      <td>walmart</td>\n",
              "      <td>229</td>\n",
              "      <td>259962</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>1226.034058</td>\n",
              "      <td>839.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>2847916030</td>\n",
              "      <td>2023-08-09</td>\n",
              "      <td>3683</td>\n",
              "      <td>walmart</td>\n",
              "      <td>225</td>\n",
              "      <td>401453</td>\n",
              "      <td>0.049922</td>\n",
              "      <td>261.916534</td>\n",
              "      <td>7821.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>2736604734</td>\n",
              "      <td>2022-11-16</td>\n",
              "      <td>1305</td>\n",
              "      <td>walmart</td>\n",
              "      <td>3852</td>\n",
              "      <td>3734991</td>\n",
              "      <td>0.114620</td>\n",
              "      <td>22.709007</td>\n",
              "      <td>1905.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index        goid       date  tokens   corpus  daily_article_count  \\\n",
              "6      6  2547227213 2019-08-02     579  walmart                  229   \n",
              "7      7  2847916030 2023-08-09    3683  walmart                  225   \n",
              "8      8  2736604734 2022-11-16    1305  walmart                 3852   \n",
              "\n",
              "   daily_token_sum  text_sentiment   text_error  text_input_tokens  \n",
              "6           259962       -0.000010  1226.034058              839.0  \n",
              "7           401453        0.049922   261.916534             7821.0  \n",
              "8          3734991        0.114620    22.709007             1905.0  "
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_parquet(f\"{analyzed_files_path}/walmart_003.parquet\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc3PTubz41kp"
      },
      "outputs": [],
      "source": [
        "sample_count = 3\n",
        "\n",
        "tqdm.pandas(desc=\"Progress\")\n",
        "\n",
        "for corpus in [\"lululemon\"]:\n",
        "#for corpus in corpora:\n",
        "    df = concat_csvs(corpus)\n",
        "    sample_df = df.sample(n=sample_count, random_state=42)\n",
        "    sample_df[[\n",
        "        \"text_sentiment\",\n",
        "        \"text_error\",\n",
        "        \"text_input_tokens\",\n",
        "    ]] = sample_df.progress_apply(lambda row: analyze_row(row), axis=1, result_type=\"expand\")\n",
        "#     file_path = f\"{analyzed_csv_path}/{corpus}.csv\"\n",
        "#     sample_df.to_csv(file_path)\n",
        "#     print(f\"Wrote {file_path}\")\n",
        "\n",
        "sample_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZY0uDBLtomE"
      },
      "outputs": [],
      "source": [
        "# Iterate\n",
        "\n",
        "\n",
        "for corpus in corpora:\n",
        "    for file in output_files:\n",
        "        if corpus in file:\n",
        "            df = pd.read_csv(f\"{csv_path}/{file}\")\n",
        "            # Add new columns\n",
        "\n",
        "            #token_breakpoint[corpus]\n",
        "\n",
        "            file_path = f\"{analyzed_csv_path}/{file}\"\n",
        "            df.to_csv(file_path)\n",
        "            print(f\"Wrote {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwpR8Fbf41kq"
      },
      "outputs": [],
      "source": [
        "# parquet\n",
        "\n",
        "\n",
        "\n",
        "for corpus in corpora:\n",
        "    file_path = f\"{analyzed_parquet_path}/{corpus}.parquet\"\n",
        "    df = pd.DataFrame()\n",
        "    for file in output_files:\n",
        "        if corpus in file:\n",
        "            df_chunk = pd.read_csv(f\"{analyzed_csv_path}/{file}\")\n",
        "            df = pd.concat([df, df_chunk], ignore_index=True)\n",
        "    df.to_parquet(file_path)\n",
        "    print(f\"Wrote {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEDJnksE41kq"
      },
      "outputs": [],
      "source": [
        "n = np.linspace(1, -1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVH-hjeh41kq"
      },
      "outputs": [],
      "source": [
        "np.average(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3_VNXCt41kq"
      },
      "outputs": [],
      "source": [
        "np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YnFPrtK41kq"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG5GNRlm41kq"
      },
      "outputs": [],
      "source": [
        "!aws s3 cp ./analyze-texts.ipynb s3://pq-tdm-studio-results/tdm-ale-data/1876/results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEyFDZ0R41kq"
      },
      "outputs": [],
      "source": [
        "foo_orig = pd.DataFrame({\n",
        "    \"index\": [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    \"foo\": [\"bar0\", \"bar1\", \"bar2\", \"bar3\", \"bar4\", \"bar5\", \"bar6\", \"bar7\", \"bar8\"]\n",
        "})\n",
        "\n",
        "foo_orig.to_parquet(f\"{full_parquets_path}/foo.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCZkY56i41kq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}